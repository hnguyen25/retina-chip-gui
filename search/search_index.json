{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"general/","text":"DC 1 Visualization Introduction Welcome! This website contains documentation for the software GUI for the new Retina Chip being developed as part of the Stanford Artificial Retina Project (ARP). Quick Links For access to the codebase: Github Repository For instructions on how to use the GUI: Intro to the GUI To setup development environment: Setup Contributors: Huy Nguyen (maintainer of wiki, contact me @ nguyen5h@stanford.edu) Maddy Hays (main advisor) John Bailey (Spring & Summer 2022) Emily Bunnapradist (Fall 2022) Sahil Adhawade (Fall 2022)","title":"Introduction"},{"location":"general/#dc-1-visualization","text":"","title":"DC 1 Visualization"},{"location":"general/#introduction","text":"Welcome! This website contains documentation for the software GUI for the new Retina Chip being developed as part of the Stanford Artificial Retina Project (ARP).","title":"Introduction"},{"location":"general/#quick-links","text":"For access to the codebase: Github Repository For instructions on how to use the GUI: Intro to the GUI To setup development environment: Setup","title":"Quick Links"},{"location":"general/#contributors","text":"Huy Nguyen (maintainer of wiki, contact me @ nguyen5h@stanford.edu) Maddy Hays (main advisor) John Bailey (Spring & Summer 2022) Emily Bunnapradist (Fall 2022) Sahil Adhawade (Fall 2022)","title":"Contributors:"},{"location":"general/getting-started/","text":"Development Getting Started Guide Note: If you would like to know how to run a compiled version of the app, refer to Intro to the GUI 1. Setting up the development environment The retina chip GUI is built completely in Python, the GUI is written with PyQt5 , the plots with PyQtGraph , and the data is manipulated through a combination of numpy , pandas , and scipy . Any IDE which can be used to edit Python code is suitable for development purposes. a. Conda environment Install and setup Anaconda . Within the Github Repository , the file dc1_vis.yml contains a list of all the packages needed to set up the development environment. A new conda environment with all the necessary packages can be installed with the command: conda env create --name rc1-gui --file dc1_vis.yml Note: this package list contains the relevant packages to run on MacOS + Apple Silicon (Huy's current development environment.) If certain packages don't install properly, this is probably because there are platform (i.e. Windows, MacOS Intel) specific packages for them. Just conda install <insert-package-name> the packages that do not install properly. b. Github Repository Clone code from the Github Repository . If you do not have access, contact nguyen5h@stanford.edu. Additional Information: PyQt and PyQtGraph Use PyQt5 (the latest version) and PyQtGraph (specifically the v0.12 builds). For more information about the library, refers to these useful links: PyQt5 Official Documentation : https://doc.qt.io/qtforpython/ PyQt5 Useful Tutorials: : https://www.pythonguis.com/pyqt5/ PyQtGraph Official Documentation : https://pyqtgraph.readthedocs.io/en/latest/developer_guide/index.html 2. Running developer build for the first time This GUI can be run either within the terminal or inside a Python IDE such as PyCharm. a. To run in terminal On new terminal session, conda activate <name-of-env> to load relevant Python libraries Navigate to the location of run.py within the cloned repository Execute command python3 run.py b. To run in PyCharm The top-right bar should contain drop down menu to choose run configuration. Click on Edit configurations... Create a new configuration or edit an existing one Change the Script path to the location of run.py , i.e. /dc1DataVis/app/run.py Setup Python interpreter to be the Conda environment which you setup in part 1 The GUI should load when you press the play button now 3. Notes on important files and where files are located (last updated Nov 2022) Executing dc1DataVis/app/run.py will startup the application. The beginning of the file contains editable parameters that will be passed through to the application ( MainWindow.py ) for the session, which can be modified by the developer. It will also be possible to toggle these parameters within the GUI in the future. Other than run.py , the entire codebase for the GUI is located within the folder dc1DataVis/app/src . src/MainWindow.py The main application for the entire GUI! Every other script links to this file! If unsure what a file does, check where it is loaded in relation to this file. Check this link for additional documentation: TODO src/data folder This file contains functions for loading, manipulating, filtering, and analyzing data collected from the retina chip. It also contains DC1DataContainer.py , which is the main class which holds all of the data visualized by the GUI during runtime. Within this data class, there are two types of data: data that is indexed by the electrode (i.e. noise, time updated, spike rate, etc.) and data that is indexed by the sequential order which data packets are received from the retina chip through the FPGA. src/gui folder The folder contains all the code to plot the different types of visualizations (noise, trace, etc.) src/layouts folder This folder contains all files with endings *.ui. These are layout files which is interpreted by PyQt5 in order to generate the different GUIs. All of these files can be viewed and edited by Qt Designer. Associated .py files with the same filename generate from these .ui files and are convenient to know how to reference different pyqtgraph elements. Official Documentation for Qt Designer : https://doc.qt.io/qt-6/qtdesigner-manual.html src/debug folder Nothing important in this folder (yet!). Will be used to contain unit testing scripts!","title":"Getting Started"},{"location":"general/getting-started/#development-getting-started-guide","text":"Note: If you would like to know how to run a compiled version of the app, refer to Intro to the GUI","title":"Development Getting Started Guide"},{"location":"general/getting-started/#1-setting-up-the-development-environment","text":"The retina chip GUI is built completely in Python, the GUI is written with PyQt5 , the plots with PyQtGraph , and the data is manipulated through a combination of numpy , pandas , and scipy . Any IDE which can be used to edit Python code is suitable for development purposes.","title":"1. Setting up the development environment"},{"location":"general/getting-started/#a-conda-environment","text":"Install and setup Anaconda . Within the Github Repository , the file dc1_vis.yml contains a list of all the packages needed to set up the development environment. A new conda environment with all the necessary packages can be installed with the command: conda env create --name rc1-gui --file dc1_vis.yml Note: this package list contains the relevant packages to run on MacOS + Apple Silicon (Huy's current development environment.) If certain packages don't install properly, this is probably because there are platform (i.e. Windows, MacOS Intel) specific packages for them. Just conda install <insert-package-name> the packages that do not install properly.","title":"a. Conda environment"},{"location":"general/getting-started/#b-github-repository","text":"Clone code from the Github Repository . If you do not have access, contact nguyen5h@stanford.edu.","title":"b. Github Repository"},{"location":"general/getting-started/#additional-information-pyqt-and-pyqtgraph","text":"Use PyQt5 (the latest version) and PyQtGraph (specifically the v0.12 builds). For more information about the library, refers to these useful links: PyQt5 Official Documentation : https://doc.qt.io/qtforpython/ PyQt5 Useful Tutorials: : https://www.pythonguis.com/pyqt5/ PyQtGraph Official Documentation : https://pyqtgraph.readthedocs.io/en/latest/developer_guide/index.html","title":"Additional Information: PyQt and PyQtGraph"},{"location":"general/getting-started/#2-running-developer-build-for-the-first-time","text":"This GUI can be run either within the terminal or inside a Python IDE such as PyCharm.","title":"2. Running developer build for the first time"},{"location":"general/getting-started/#a-to-run-in-terminal","text":"On new terminal session, conda activate <name-of-env> to load relevant Python libraries Navigate to the location of run.py within the cloned repository Execute command python3 run.py","title":"a. To run in terminal"},{"location":"general/getting-started/#b-to-run-in-pycharm","text":"The top-right bar should contain drop down menu to choose run configuration. Click on Edit configurations... Create a new configuration or edit an existing one Change the Script path to the location of run.py , i.e. /dc1DataVis/app/run.py Setup Python interpreter to be the Conda environment which you setup in part 1 The GUI should load when you press the play button now","title":"b. To run in PyCharm"},{"location":"general/getting-started/#3-notes-on-important-files-and-where-files-are-located-last-updated-nov-2022","text":"Executing dc1DataVis/app/run.py will startup the application. The beginning of the file contains editable parameters that will be passed through to the application ( MainWindow.py ) for the session, which can be modified by the developer. It will also be possible to toggle these parameters within the GUI in the future. Other than run.py , the entire codebase for the GUI is located within the folder dc1DataVis/app/src .","title":"3. Notes on important files and where files are located (last updated Nov 2022)"},{"location":"general/getting-started/#srcmainwindowpy","text":"The main application for the entire GUI! Every other script links to this file! If unsure what a file does, check where it is loaded in relation to this file. Check this link for additional documentation: TODO","title":"src/MainWindow.py"},{"location":"general/getting-started/#srcdata-folder","text":"This file contains functions for loading, manipulating, filtering, and analyzing data collected from the retina chip. It also contains DC1DataContainer.py , which is the main class which holds all of the data visualized by the GUI during runtime. Within this data class, there are two types of data: data that is indexed by the electrode (i.e. noise, time updated, spike rate, etc.) and data that is indexed by the sequential order which data packets are received from the retina chip through the FPGA.","title":"src/data folder"},{"location":"general/getting-started/#srcgui-folder","text":"The folder contains all the code to plot the different types of visualizations (noise, trace, etc.)","title":"src/gui folder"},{"location":"general/getting-started/#srclayouts-folder","text":"This folder contains all files with endings *.ui. These are layout files which is interpreted by PyQt5 in order to generate the different GUIs. All of these files can be viewed and edited by Qt Designer. Associated .py files with the same filename generate from these .ui files and are convenient to know how to reference different pyqtgraph elements. Official Documentation for Qt Designer : https://doc.qt.io/qt-6/qtdesigner-manual.html","title":"src/layouts folder"},{"location":"general/getting-started/#srcdebug-folder","text":"Nothing important in this folder (yet!). Will be used to contain unit testing scripts!","title":"src/debug folder"},{"location":"general/intro-to-gui/","text":"Introduction to Retina Chip GUI 1. Downloading application The latest version of the rc-gui is available here: 2. Acceptable Data Formats 3. Opening the application 4. Choose session parameters 5. Types of Visualizations a. Spike Finding Plots b. Noise Plots c. Spike Search Plots d. Diagnostic Plot 6. Additional Tools 7. Debugging and Profiling the GUI","title":"Introduction to Retina Chip GUI"},{"location":"general/intro-to-gui/#introduction-to-retina-chip-gui","text":"","title":"Introduction to Retina Chip GUI"},{"location":"general/intro-to-gui/#1-downloading-application","text":"The latest version of the rc-gui is available here:","title":"1. Downloading application"},{"location":"general/intro-to-gui/#2-acceptable-data-formats","text":"","title":"2. Acceptable Data Formats"},{"location":"general/intro-to-gui/#3-opening-the-application","text":"","title":"3. Opening the application"},{"location":"general/intro-to-gui/#4-choose-session-parameters","text":"","title":"4. Choose session parameters"},{"location":"general/intro-to-gui/#5-types-of-visualizations","text":"","title":"5. Types of Visualizations"},{"location":"general/intro-to-gui/#a-spike-finding-plots","text":"","title":"a. Spike Finding Plots"},{"location":"general/intro-to-gui/#b-noise-plots","text":"","title":"b. Noise Plots"},{"location":"general/intro-to-gui/#c-spike-search-plots","text":"","title":"c. Spike Search Plots"},{"location":"general/intro-to-gui/#d-diagnostic-plot","text":"","title":"d. Diagnostic Plot"},{"location":"general/intro-to-gui/#6-additional-tools","text":"","title":"6. Additional Tools"},{"location":"general/intro-to-gui/#7-debugging-and-profiling-the-gui","text":"","title":"7. Debugging and Profiling the GUI"},{"location":"model/DC1DataContainer/","text":"DC1 Data Container DC1DataContainer Container for holding recording model for the DC1 retina chip. Each container is designed to hold all the relevant information extracted from model collected from a SINGLE recording, of any particular type. To-Dos: TODO figure out time alignment of the model / the actual sampling rate / check for dropped packets TODO figure out time budget for model processing + filtering Source code in src/model/DC1DataContainer.py class DC1DataContainer : \"\"\" Container for holding recording model for the DC1 retina chip. Each container is designed to hold all the relevant information extracted from model collected from a SINGLE recording, of any particular type. To-Dos: ---------- TODO figure out time alignment of the model / the actual sampling rate / check for dropped packets TODO figure out time budget for model processing + filtering \"\"\" # +++++ CONSTANTS +++++ # DC1/RC1.5 is a multi-electrode array (MEA) with 32 x 32 channels (1024 total) ARRAY_NUM_ROWS , ARRAY_NUM_COLS = 32 , 32 count_track , time_track = None , None # calculated statistics spike_data = { 'times' : np . zeros (( 32 , 32 )), # np.array with dims 32 x 32 x num_bins 'amplitude' : np . zeros (( 32 , 32 )) } # new model structs to_serialize , to_show = None , None avg_spike_rate_x = [] avg_spike_rate_y = [] def __init__ ( self , app , recording_info = {}, data_processing_settings = {}): \"\"\" Args: app: recording_info: data_processing_settings: \"\"\" self . app = app # reference to MainWindow self . time_track , self . count_track = 0 , 0 self . time_track_processed , self . count_track_processed = 0 , 0 # channel-level information # indexed via row, col of array # value: a dict containing all info for a certain electrode # self.array_indexed_df = pd.Dataframe() # TODO make array-indexed pandas df to replace below df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"channel_noise_mean\" , \"channel_noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" , #spikes \"array_dot_color\" , \"array_dot_size\" # specific plot info ] initial_data = [] for i in range ( 32 ): for j in range ( 32 ): initial_data . append ([ i , j , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) self . df = pd . DataFrame ( initial_data , columns = df_columns ) self . stats = { \"largest_spike_cnt\" : 0 } self . array_spike_times = { \"spike_bins\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )], \"spike_bins_max_amps\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )] } # ===================== # buffer-level information # indexed via key: buffer number (0 - MAX_NUMBER_OF_BUFFERS) # value: self . buffer_indexed = [] self . to_serialize = queue . Queue () self . to_show = queue . Queue () def append_buf ( self , buf ): \"\"\" Args: buf: Returns: \"\"\" packet_idx = buf [ 'packet_idx' ] channel_idxs = [] # for buffer_indexed model struct # calculate times N = buf [ \"packet_data\" ][ 0 ][ \"N\" ] len_packet_time = N * 0.05 # 20kHz sampling rate, means time_recording (ms) = num_sam*0.05ms next_times = np . linspace ( self . time_track , self . time_track + len_packet_time , N ) self . time_track += len_packet_time self . count_track += N avg_packet_spike_count = 0 packet_data = buf [ 'packet_data' ] for packet in packet_data : # load model # packet keys: 'data_real', 'cnt_real', 'N', # 'channel_idx', 'preprocessed_data', 'filtered_data', # 'stats_cnt', 'stats_noise+mean', 'stats_noise+std' this_channel_idx = packet [ 'channel_idx' ] # for buffer_indexed model struct channel_idxs . append ( this_channel_idx ) # reformatting packet for to_show model struct packet [ \"times\" ] = next_times # for array_indexed model struct r , c = idx2map ( this_channel_idx ) # TODO make this adapt for when multiple packets record from the same channel # use formula Maddy gave df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"noise_mean\" , \"noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" ] # spikes self . df . at [ this_channel_idx , \"N\" ] += packet [ \"stats_cnt\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"noise_mean\" ] = packet [ \"stats_noise+mean\" ] self . df . at [ this_channel_idx , \"noise_std\" ] = packet [ \"stats_noise+std\" ] self . df . at [ this_channel_idx , \"buf_recording_len\" ] = packet [ \"stats_buf+recording+len\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"spikes_cnt\" ] = packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_cum_cnt\" ] += packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_avg_amp\" ] += packet [ \"stats_spikes+avg+amp\" ] self . df . at [ this_channel_idx , \"spikes_std\" ] += packet [ \"stats_spikes+std\" ] \"\"\" self.array_indexed = { \"stats_num+spike+bins+in+buffer\": np.zeros((self.ARRAY_NUM_ROWS, self.ARRAY_NUM_COLS)), \"spike_bins\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)], \"spike_bins_max_amps\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)] } \"\"\" # TODO put spike bins here #print('array spike times -> spike_bins', packet[\"spike_bins\"]) #print('array spike times -> spike_bins_max_amp', packet[\"spike_bins_max_amps\"]) self . array_spike_times [ \"spike_bins\" ][ this_channel_idx ] = packet [ \"spike_bins\" ] self . array_spike_times [ \"spike_bins_max_amps\" ][ this_channel_idx ] = packet [ \"spike_bins_max_amps\" ] avg_packet_spike_count += packet [ \"stats_spikes+cnt\" ] # buffer-level information buffer_indexed_dict = { \"file_dir\" : buf [ \"file_dir\" ], \"filter_type\" : buf [ \"filter_type\" ], \"N\" : N , \"time_elapsed\" : len_packet_time , # TODO check if this is accurate for all recording types \"channel_idxs\" : channel_idxs , \"num_detected_spikes\" : avg_packet_spike_count / len ( packet_data ) # for spike rate plot } self . buffer_indexed . append ( buffer_indexed_dict ) self . calculate_moving_spike_rate_avg () self . to_show . put ( buf ) # return the channels in the buffer return buffer_indexed_dict [ \"channel_idxs\" ] def calculate_moving_spike_rate_avg ( self ): \"\"\" Returns: \"\"\" avg_spike_rate = 0 time_elapsed = 0 SPIKE_RATE_WINDOW_SIZE = 4 if len ( self . buffer_indexed ) < SPIKE_RATE_WINDOW_SIZE : for buffer in self . buffer_indexed : avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] else : for buffer in self . buffer_indexed [ - 1 - SPIKE_RATE_WINDOW_SIZE : - 1 ]: avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] #print('avg spike rate before division', avg_spike_rate) avg_spike_rate /= time_elapsed x = self . time_track y = avg_spike_rate self . avg_spike_rate_x . append ( x ) self . avg_spike_rate_y . append ( y ) def find_last_buffer_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" # return buffer which contains an electrode idx # start from the end len_processed_buffer = len ( self . buffer_indexed ) for i in reversed ( range ( len_processed_buffer )): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: return i return - 1 def find_all_buffers_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffers = [] len_processed_buffer = len ( self . buffer_indexed ) for i in range ( len_processed_buffer ): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: buffers . append ( i ) return buffers def get_last_trace_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffer_idx = self . find_last_buffer_with_electrode_idx ( electrode_idx ) if buffer_idx == - 1 : return None , None else : params = { \"file_dir\" : self . buffer_indexed [ buffer_idx ][ \"file_dir\" ], \"filter_type\" : self . buffer_indexed [ buffer_idx ][ \"filter_type\" ], \"SPIKING_THRESHOLD\" : self . app . settings [ \"spikeThreshold\" ], \"BIN_SIZE\" : self . app . settings [ \"binSize\" ] } N , Y = None , None packet = load_one_mat_file ( params ) for channel_data in packet [ \"packet_data\" ]: if channel_data [ \"channel_idx\" ] == electrode_idx : Y = channel_data [ \"filtered_data\" ] N = channel_data [ \"N\" ] time_elapsed = self . buffer_indexed [ buffer_idx ][ \"time_elapsed\" ] SAMPLING_PERIOD = 0.05 # check data_loading.py for more details end_time = N * SAMPLING_PERIOD X = np . linspace ( time_elapsed , time_elapsed + end_time , N + 1 ) return X , Y __init__ ( self , app , recording_info = {}, data_processing_settings = {}) special Parameters: Name Type Description Default app required recording_info {} data_processing_settings {} Source code in src/model/DC1DataContainer.py def __init__ ( self , app , recording_info = {}, data_processing_settings = {}): \"\"\" Args: app: recording_info: data_processing_settings: \"\"\" self . app = app # reference to MainWindow self . time_track , self . count_track = 0 , 0 self . time_track_processed , self . count_track_processed = 0 , 0 # channel-level information # indexed via row, col of array # value: a dict containing all info for a certain electrode # self.array_indexed_df = pd.Dataframe() # TODO make array-indexed pandas df to replace below df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"channel_noise_mean\" , \"channel_noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" , #spikes \"array_dot_color\" , \"array_dot_size\" # specific plot info ] initial_data = [] for i in range ( 32 ): for j in range ( 32 ): initial_data . append ([ i , j , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) self . df = pd . DataFrame ( initial_data , columns = df_columns ) self . stats = { \"largest_spike_cnt\" : 0 } self . array_spike_times = { \"spike_bins\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )], \"spike_bins_max_amps\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )] } # ===================== # buffer-level information # indexed via key: buffer number (0 - MAX_NUMBER_OF_BUFFERS) # value: self . buffer_indexed = [] self . to_serialize = queue . Queue () self . to_show = queue . Queue () append_buf ( self , buf ) Parameters: Name Type Description Default buf required Source code in src/model/DC1DataContainer.py def append_buf ( self , buf ): \"\"\" Args: buf: Returns: \"\"\" packet_idx = buf [ 'packet_idx' ] channel_idxs = [] # for buffer_indexed model struct # calculate times N = buf [ \"packet_data\" ][ 0 ][ \"N\" ] len_packet_time = N * 0.05 # 20kHz sampling rate, means time_recording (ms) = num_sam*0.05ms next_times = np . linspace ( self . time_track , self . time_track + len_packet_time , N ) self . time_track += len_packet_time self . count_track += N avg_packet_spike_count = 0 packet_data = buf [ 'packet_data' ] for packet in packet_data : # load model # packet keys: 'data_real', 'cnt_real', 'N', # 'channel_idx', 'preprocessed_data', 'filtered_data', # 'stats_cnt', 'stats_noise+mean', 'stats_noise+std' this_channel_idx = packet [ 'channel_idx' ] # for buffer_indexed model struct channel_idxs . append ( this_channel_idx ) # reformatting packet for to_show model struct packet [ \"times\" ] = next_times # for array_indexed model struct r , c = idx2map ( this_channel_idx ) # TODO make this adapt for when multiple packets record from the same channel # use formula Maddy gave df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"noise_mean\" , \"noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" ] # spikes self . df . at [ this_channel_idx , \"N\" ] += packet [ \"stats_cnt\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"noise_mean\" ] = packet [ \"stats_noise+mean\" ] self . df . at [ this_channel_idx , \"noise_std\" ] = packet [ \"stats_noise+std\" ] self . df . at [ this_channel_idx , \"buf_recording_len\" ] = packet [ \"stats_buf+recording+len\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"spikes_cnt\" ] = packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_cum_cnt\" ] += packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_avg_amp\" ] += packet [ \"stats_spikes+avg+amp\" ] self . df . at [ this_channel_idx , \"spikes_std\" ] += packet [ \"stats_spikes+std\" ] \"\"\" self.array_indexed = { \"stats_num+spike+bins+in+buffer\": np.zeros((self.ARRAY_NUM_ROWS, self.ARRAY_NUM_COLS)), \"spike_bins\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)], \"spike_bins_max_amps\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)] } \"\"\" # TODO put spike bins here #print('array spike times -> spike_bins', packet[\"spike_bins\"]) #print('array spike times -> spike_bins_max_amp', packet[\"spike_bins_max_amps\"]) self . array_spike_times [ \"spike_bins\" ][ this_channel_idx ] = packet [ \"spike_bins\" ] self . array_spike_times [ \"spike_bins_max_amps\" ][ this_channel_idx ] = packet [ \"spike_bins_max_amps\" ] avg_packet_spike_count += packet [ \"stats_spikes+cnt\" ] # buffer-level information buffer_indexed_dict = { \"file_dir\" : buf [ \"file_dir\" ], \"filter_type\" : buf [ \"filter_type\" ], \"N\" : N , \"time_elapsed\" : len_packet_time , # TODO check if this is accurate for all recording types \"channel_idxs\" : channel_idxs , \"num_detected_spikes\" : avg_packet_spike_count / len ( packet_data ) # for spike rate plot } self . buffer_indexed . append ( buffer_indexed_dict ) self . calculate_moving_spike_rate_avg () self . to_show . put ( buf ) # return the channels in the buffer return buffer_indexed_dict [ \"channel_idxs\" ] calculate_moving_spike_rate_avg ( self ) Source code in src/model/DC1DataContainer.py def calculate_moving_spike_rate_avg ( self ): \"\"\" Returns: \"\"\" avg_spike_rate = 0 time_elapsed = 0 SPIKE_RATE_WINDOW_SIZE = 4 if len ( self . buffer_indexed ) < SPIKE_RATE_WINDOW_SIZE : for buffer in self . buffer_indexed : avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] else : for buffer in self . buffer_indexed [ - 1 - SPIKE_RATE_WINDOW_SIZE : - 1 ]: avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] #print('avg spike rate before division', avg_spike_rate) avg_spike_rate /= time_elapsed x = self . time_track y = avg_spike_rate self . avg_spike_rate_x . append ( x ) self . avg_spike_rate_y . append ( y ) find_all_buffers_with_electrode_idx ( self , electrode_idx ) Parameters: Name Type Description Default electrode_idx required Source code in src/model/DC1DataContainer.py def find_all_buffers_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffers = [] len_processed_buffer = len ( self . buffer_indexed ) for i in range ( len_processed_buffer ): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: buffers . append ( i ) return buffers find_last_buffer_with_electrode_idx ( self , electrode_idx ) Parameters: Name Type Description Default electrode_idx required Source code in src/model/DC1DataContainer.py def find_last_buffer_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" # return buffer which contains an electrode idx # start from the end len_processed_buffer = len ( self . buffer_indexed ) for i in reversed ( range ( len_processed_buffer )): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: return i return - 1 get_last_trace_with_electrode_idx ( self , electrode_idx ) Parameters: Name Type Description Default electrode_idx required Source code in src/model/DC1DataContainer.py def get_last_trace_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffer_idx = self . find_last_buffer_with_electrode_idx ( electrode_idx ) if buffer_idx == - 1 : return None , None else : params = { \"file_dir\" : self . buffer_indexed [ buffer_idx ][ \"file_dir\" ], \"filter_type\" : self . buffer_indexed [ buffer_idx ][ \"filter_type\" ], \"SPIKING_THRESHOLD\" : self . app . settings [ \"spikeThreshold\" ], \"BIN_SIZE\" : self . app . settings [ \"binSize\" ] } N , Y = None , None packet = load_one_mat_file ( params ) for channel_data in packet [ \"packet_data\" ]: if channel_data [ \"channel_idx\" ] == electrode_idx : Y = channel_data [ \"filtered_data\" ] N = channel_data [ \"N\" ] time_elapsed = self . buffer_indexed [ buffer_idx ][ \"time_elapsed\" ] SAMPLING_PERIOD = 0.05 # check data_loading.py for more details end_time = N * SAMPLING_PERIOD X = np . linspace ( time_elapsed , time_elapsed + end_time , N + 1 ) return X , Y map2idx ( ch_row , ch_col ) Given a channel's row and col, return channel's index Parameters: Name Type Description Default ch_row int row index of channel in array (up to 32) required ch_col int column index of channel in array (up to 32) required Returns: numerical index of array Source code in src/model/DC1DataContainer.py def map2idx ( ch_row : int , ch_col : int ): \"\"\" Given a channel's row and col, return channel's index Args: ch_row: row index of channel in array (up to 32) ch_col: column index of channel in array (up to 32) Returns: numerical index of array \"\"\" if ch_row > 31 or ch_row < 0 : print ( 'Row out of range' ) elif ch_col > 31 or ch_col < 0 : print ( 'Col out of range' ) else : ch_idx = int ( ch_row * 32 + ch_col ) return ch_idx","title":"data structs"},{"location":"model/DC1DataContainer/#dc1-data-container","text":"","title":"DC1 Data Container"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer","text":"Container for holding recording model for the DC1 retina chip. Each container is designed to hold all the relevant information extracted from model collected from a SINGLE recording, of any particular type.","title":"DC1DataContainer"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer--to-dos","text":"TODO figure out time alignment of the model / the actual sampling rate / check for dropped packets TODO figure out time budget for model processing + filtering Source code in src/model/DC1DataContainer.py class DC1DataContainer : \"\"\" Container for holding recording model for the DC1 retina chip. Each container is designed to hold all the relevant information extracted from model collected from a SINGLE recording, of any particular type. To-Dos: ---------- TODO figure out time alignment of the model / the actual sampling rate / check for dropped packets TODO figure out time budget for model processing + filtering \"\"\" # +++++ CONSTANTS +++++ # DC1/RC1.5 is a multi-electrode array (MEA) with 32 x 32 channels (1024 total) ARRAY_NUM_ROWS , ARRAY_NUM_COLS = 32 , 32 count_track , time_track = None , None # calculated statistics spike_data = { 'times' : np . zeros (( 32 , 32 )), # np.array with dims 32 x 32 x num_bins 'amplitude' : np . zeros (( 32 , 32 )) } # new model structs to_serialize , to_show = None , None avg_spike_rate_x = [] avg_spike_rate_y = [] def __init__ ( self , app , recording_info = {}, data_processing_settings = {}): \"\"\" Args: app: recording_info: data_processing_settings: \"\"\" self . app = app # reference to MainWindow self . time_track , self . count_track = 0 , 0 self . time_track_processed , self . count_track_processed = 0 , 0 # channel-level information # indexed via row, col of array # value: a dict containing all info for a certain electrode # self.array_indexed_df = pd.Dataframe() # TODO make array-indexed pandas df to replace below df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"channel_noise_mean\" , \"channel_noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" , #spikes \"array_dot_color\" , \"array_dot_size\" # specific plot info ] initial_data = [] for i in range ( 32 ): for j in range ( 32 ): initial_data . append ([ i , j , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) self . df = pd . DataFrame ( initial_data , columns = df_columns ) self . stats = { \"largest_spike_cnt\" : 0 } self . array_spike_times = { \"spike_bins\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )], \"spike_bins_max_amps\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )] } # ===================== # buffer-level information # indexed via key: buffer number (0 - MAX_NUMBER_OF_BUFFERS) # value: self . buffer_indexed = [] self . to_serialize = queue . Queue () self . to_show = queue . Queue () def append_buf ( self , buf ): \"\"\" Args: buf: Returns: \"\"\" packet_idx = buf [ 'packet_idx' ] channel_idxs = [] # for buffer_indexed model struct # calculate times N = buf [ \"packet_data\" ][ 0 ][ \"N\" ] len_packet_time = N * 0.05 # 20kHz sampling rate, means time_recording (ms) = num_sam*0.05ms next_times = np . linspace ( self . time_track , self . time_track + len_packet_time , N ) self . time_track += len_packet_time self . count_track += N avg_packet_spike_count = 0 packet_data = buf [ 'packet_data' ] for packet in packet_data : # load model # packet keys: 'data_real', 'cnt_real', 'N', # 'channel_idx', 'preprocessed_data', 'filtered_data', # 'stats_cnt', 'stats_noise+mean', 'stats_noise+std' this_channel_idx = packet [ 'channel_idx' ] # for buffer_indexed model struct channel_idxs . append ( this_channel_idx ) # reformatting packet for to_show model struct packet [ \"times\" ] = next_times # for array_indexed model struct r , c = idx2map ( this_channel_idx ) # TODO make this adapt for when multiple packets record from the same channel # use formula Maddy gave df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"noise_mean\" , \"noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" ] # spikes self . df . at [ this_channel_idx , \"N\" ] += packet [ \"stats_cnt\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"noise_mean\" ] = packet [ \"stats_noise+mean\" ] self . df . at [ this_channel_idx , \"noise_std\" ] = packet [ \"stats_noise+std\" ] self . df . at [ this_channel_idx , \"buf_recording_len\" ] = packet [ \"stats_buf+recording+len\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"spikes_cnt\" ] = packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_cum_cnt\" ] += packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_avg_amp\" ] += packet [ \"stats_spikes+avg+amp\" ] self . df . at [ this_channel_idx , \"spikes_std\" ] += packet [ \"stats_spikes+std\" ] \"\"\" self.array_indexed = { \"stats_num+spike+bins+in+buffer\": np.zeros((self.ARRAY_NUM_ROWS, self.ARRAY_NUM_COLS)), \"spike_bins\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)], \"spike_bins_max_amps\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)] } \"\"\" # TODO put spike bins here #print('array spike times -> spike_bins', packet[\"spike_bins\"]) #print('array spike times -> spike_bins_max_amp', packet[\"spike_bins_max_amps\"]) self . array_spike_times [ \"spike_bins\" ][ this_channel_idx ] = packet [ \"spike_bins\" ] self . array_spike_times [ \"spike_bins_max_amps\" ][ this_channel_idx ] = packet [ \"spike_bins_max_amps\" ] avg_packet_spike_count += packet [ \"stats_spikes+cnt\" ] # buffer-level information buffer_indexed_dict = { \"file_dir\" : buf [ \"file_dir\" ], \"filter_type\" : buf [ \"filter_type\" ], \"N\" : N , \"time_elapsed\" : len_packet_time , # TODO check if this is accurate for all recording types \"channel_idxs\" : channel_idxs , \"num_detected_spikes\" : avg_packet_spike_count / len ( packet_data ) # for spike rate plot } self . buffer_indexed . append ( buffer_indexed_dict ) self . calculate_moving_spike_rate_avg () self . to_show . put ( buf ) # return the channels in the buffer return buffer_indexed_dict [ \"channel_idxs\" ] def calculate_moving_spike_rate_avg ( self ): \"\"\" Returns: \"\"\" avg_spike_rate = 0 time_elapsed = 0 SPIKE_RATE_WINDOW_SIZE = 4 if len ( self . buffer_indexed ) < SPIKE_RATE_WINDOW_SIZE : for buffer in self . buffer_indexed : avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] else : for buffer in self . buffer_indexed [ - 1 - SPIKE_RATE_WINDOW_SIZE : - 1 ]: avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] #print('avg spike rate before division', avg_spike_rate) avg_spike_rate /= time_elapsed x = self . time_track y = avg_spike_rate self . avg_spike_rate_x . append ( x ) self . avg_spike_rate_y . append ( y ) def find_last_buffer_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" # return buffer which contains an electrode idx # start from the end len_processed_buffer = len ( self . buffer_indexed ) for i in reversed ( range ( len_processed_buffer )): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: return i return - 1 def find_all_buffers_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffers = [] len_processed_buffer = len ( self . buffer_indexed ) for i in range ( len_processed_buffer ): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: buffers . append ( i ) return buffers def get_last_trace_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffer_idx = self . find_last_buffer_with_electrode_idx ( electrode_idx ) if buffer_idx == - 1 : return None , None else : params = { \"file_dir\" : self . buffer_indexed [ buffer_idx ][ \"file_dir\" ], \"filter_type\" : self . buffer_indexed [ buffer_idx ][ \"filter_type\" ], \"SPIKING_THRESHOLD\" : self . app . settings [ \"spikeThreshold\" ], \"BIN_SIZE\" : self . app . settings [ \"binSize\" ] } N , Y = None , None packet = load_one_mat_file ( params ) for channel_data in packet [ \"packet_data\" ]: if channel_data [ \"channel_idx\" ] == electrode_idx : Y = channel_data [ \"filtered_data\" ] N = channel_data [ \"N\" ] time_elapsed = self . buffer_indexed [ buffer_idx ][ \"time_elapsed\" ] SAMPLING_PERIOD = 0.05 # check data_loading.py for more details end_time = N * SAMPLING_PERIOD X = np . linspace ( time_elapsed , time_elapsed + end_time , N + 1 ) return X , Y","title":"To-Dos:"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer.__init__","text":"Parameters: Name Type Description Default app required recording_info {} data_processing_settings {} Source code in src/model/DC1DataContainer.py def __init__ ( self , app , recording_info = {}, data_processing_settings = {}): \"\"\" Args: app: recording_info: data_processing_settings: \"\"\" self . app = app # reference to MainWindow self . time_track , self . count_track = 0 , 0 self . time_track_processed , self . count_track_processed = 0 , 0 # channel-level information # indexed via row, col of array # value: a dict containing all info for a certain electrode # self.array_indexed_df = pd.Dataframe() # TODO make array-indexed pandas df to replace below df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"channel_noise_mean\" , \"channel_noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" , #spikes \"array_dot_color\" , \"array_dot_size\" # specific plot info ] initial_data = [] for i in range ( 32 ): for j in range ( 32 ): initial_data . append ([ i , j , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) self . df = pd . DataFrame ( initial_data , columns = df_columns ) self . stats = { \"largest_spike_cnt\" : 0 } self . array_spike_times = { \"spike_bins\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )], \"spike_bins_max_amps\" : [[] for i in range ( self . ARRAY_NUM_ROWS * self . ARRAY_NUM_COLS )] } # ===================== # buffer-level information # indexed via key: buffer number (0 - MAX_NUMBER_OF_BUFFERS) # value: self . buffer_indexed = [] self . to_serialize = queue . Queue () self . to_show = queue . Queue ()","title":"__init__()"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer.append_buf","text":"Parameters: Name Type Description Default buf required Source code in src/model/DC1DataContainer.py def append_buf ( self , buf ): \"\"\" Args: buf: Returns: \"\"\" packet_idx = buf [ 'packet_idx' ] channel_idxs = [] # for buffer_indexed model struct # calculate times N = buf [ \"packet_data\" ][ 0 ][ \"N\" ] len_packet_time = N * 0.05 # 20kHz sampling rate, means time_recording (ms) = num_sam*0.05ms next_times = np . linspace ( self . time_track , self . time_track + len_packet_time , N ) self . time_track += len_packet_time self . count_track += N avg_packet_spike_count = 0 packet_data = buf [ 'packet_data' ] for packet in packet_data : # load model # packet keys: 'data_real', 'cnt_real', 'N', # 'channel_idx', 'preprocessed_data', 'filtered_data', # 'stats_cnt', 'stats_noise+mean', 'stats_noise+std' this_channel_idx = packet [ 'channel_idx' ] # for buffer_indexed model struct channel_idxs . append ( this_channel_idx ) # reformatting packet for to_show model struct packet [ \"times\" ] = next_times # for array_indexed model struct r , c = idx2map ( this_channel_idx ) # TODO make this adapt for when multiple packets record from the same channel # use formula Maddy gave df_columns = [ \"row\" , \"col\" , # indexing \"avg_filtered_amp\" , \"avg_unfiltered_amp\" , \"noise_mean\" , \"noise_std\" , # noise \"start_time\" , \"start_count\" , \"buf_recording_len\" , \"N\" , \"packet_idx\" , # timing \"spikes_avg_amp\" , \"spikes_cnt\" , \"spikes_std\" , \"spikes_cum_cnt\" , \"num_bins_per_buffer\" ] # spikes self . df . at [ this_channel_idx , \"N\" ] += packet [ \"stats_cnt\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"noise_mean\" ] = packet [ \"stats_noise+mean\" ] self . df . at [ this_channel_idx , \"noise_std\" ] = packet [ \"stats_noise+std\" ] self . df . at [ this_channel_idx , \"buf_recording_len\" ] = packet [ \"stats_buf+recording+len\" ] self . df . at [ this_channel_idx , \"avg_unfiltered_amp\" ] = packet [ \"stats_avg+unfiltered+amp\" ] self . df . at [ this_channel_idx , \"spikes_cnt\" ] = packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_cum_cnt\" ] += packet [ \"stats_spikes+cnt\" ] self . df . at [ this_channel_idx , \"spikes_avg_amp\" ] += packet [ \"stats_spikes+avg+amp\" ] self . df . at [ this_channel_idx , \"spikes_std\" ] += packet [ \"stats_spikes+std\" ] \"\"\" self.array_indexed = { \"stats_num+spike+bins+in+buffer\": np.zeros((self.ARRAY_NUM_ROWS, self.ARRAY_NUM_COLS)), \"spike_bins\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)], \"spike_bins_max_amps\": [([] for i in range(self.ARRAY_NUM_ROWS)) for j in range(self.ARRAY_NUM_COLS)] } \"\"\" # TODO put spike bins here #print('array spike times -> spike_bins', packet[\"spike_bins\"]) #print('array spike times -> spike_bins_max_amp', packet[\"spike_bins_max_amps\"]) self . array_spike_times [ \"spike_bins\" ][ this_channel_idx ] = packet [ \"spike_bins\" ] self . array_spike_times [ \"spike_bins_max_amps\" ][ this_channel_idx ] = packet [ \"spike_bins_max_amps\" ] avg_packet_spike_count += packet [ \"stats_spikes+cnt\" ] # buffer-level information buffer_indexed_dict = { \"file_dir\" : buf [ \"file_dir\" ], \"filter_type\" : buf [ \"filter_type\" ], \"N\" : N , \"time_elapsed\" : len_packet_time , # TODO check if this is accurate for all recording types \"channel_idxs\" : channel_idxs , \"num_detected_spikes\" : avg_packet_spike_count / len ( packet_data ) # for spike rate plot } self . buffer_indexed . append ( buffer_indexed_dict ) self . calculate_moving_spike_rate_avg () self . to_show . put ( buf ) # return the channels in the buffer return buffer_indexed_dict [ \"channel_idxs\" ]","title":"append_buf()"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer.calculate_moving_spike_rate_avg","text":"Source code in src/model/DC1DataContainer.py def calculate_moving_spike_rate_avg ( self ): \"\"\" Returns: \"\"\" avg_spike_rate = 0 time_elapsed = 0 SPIKE_RATE_WINDOW_SIZE = 4 if len ( self . buffer_indexed ) < SPIKE_RATE_WINDOW_SIZE : for buffer in self . buffer_indexed : avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] else : for buffer in self . buffer_indexed [ - 1 - SPIKE_RATE_WINDOW_SIZE : - 1 ]: avg_spike_rate += buffer [ \"num_detected_spikes\" ] time_elapsed += buffer [ \"time_elapsed\" ] #print('avg spike rate before division', avg_spike_rate) avg_spike_rate /= time_elapsed x = self . time_track y = avg_spike_rate self . avg_spike_rate_x . append ( x ) self . avg_spike_rate_y . append ( y )","title":"calculate_moving_spike_rate_avg()"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer.find_all_buffers_with_electrode_idx","text":"Parameters: Name Type Description Default electrode_idx required Source code in src/model/DC1DataContainer.py def find_all_buffers_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffers = [] len_processed_buffer = len ( self . buffer_indexed ) for i in range ( len_processed_buffer ): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: buffers . append ( i ) return buffers","title":"find_all_buffers_with_electrode_idx()"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer.find_last_buffer_with_electrode_idx","text":"Parameters: Name Type Description Default electrode_idx required Source code in src/model/DC1DataContainer.py def find_last_buffer_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" # return buffer which contains an electrode idx # start from the end len_processed_buffer = len ( self . buffer_indexed ) for i in reversed ( range ( len_processed_buffer )): if electrode_idx in self . buffer_indexed [ i ][ \"channel_idxs\" ]: return i return - 1","title":"find_last_buffer_with_electrode_idx()"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.DC1DataContainer.get_last_trace_with_electrode_idx","text":"Parameters: Name Type Description Default electrode_idx required Source code in src/model/DC1DataContainer.py def get_last_trace_with_electrode_idx ( self , electrode_idx ): \"\"\" Args: electrode_idx: Returns: \"\"\" buffer_idx = self . find_last_buffer_with_electrode_idx ( electrode_idx ) if buffer_idx == - 1 : return None , None else : params = { \"file_dir\" : self . buffer_indexed [ buffer_idx ][ \"file_dir\" ], \"filter_type\" : self . buffer_indexed [ buffer_idx ][ \"filter_type\" ], \"SPIKING_THRESHOLD\" : self . app . settings [ \"spikeThreshold\" ], \"BIN_SIZE\" : self . app . settings [ \"binSize\" ] } N , Y = None , None packet = load_one_mat_file ( params ) for channel_data in packet [ \"packet_data\" ]: if channel_data [ \"channel_idx\" ] == electrode_idx : Y = channel_data [ \"filtered_data\" ] N = channel_data [ \"N\" ] time_elapsed = self . buffer_indexed [ buffer_idx ][ \"time_elapsed\" ] SAMPLING_PERIOD = 0.05 # check data_loading.py for more details end_time = N * SAMPLING_PERIOD X = np . linspace ( time_elapsed , time_elapsed + end_time , N + 1 ) return X , Y","title":"get_last_trace_with_electrode_idx()"},{"location":"model/DC1DataContainer/#src.model.DC1DataContainer.map2idx","text":"Given a channel's row and col, return channel's index Parameters: Name Type Description Default ch_row int row index of channel in array (up to 32) required ch_col int column index of channel in array (up to 32) required Returns: numerical index of array Source code in src/model/DC1DataContainer.py def map2idx ( ch_row : int , ch_col : int ): \"\"\" Given a channel's row and col, return channel's index Args: ch_row: row index of channel in array (up to 32) ch_col: column index of channel in array (up to 32) Returns: numerical index of array \"\"\" if ch_row > 31 or ch_row < 0 : print ( 'Row out of range' ) elif ch_col > 31 or ch_col < 0 : print ( 'Col out of range' ) else : ch_idx = int ( ch_row * 32 + ch_col ) return ch_idx","title":"map2idx()"},{"location":"model/data_loading/","text":"Data Loading Functions for loading model files into form that can be read by GUI idx2map ( ch_idx ) Given a channel index, return the channel's row and col Parameters: Name Type Description Default ch_idx int single numerical index for array (up to 1024) required Returns: Type Description channel row and channel index Source code in src/model/data_loading.py def idx2map ( ch_idx : int ): \"\"\" Given a channel index, return the channel's row and col Args: ch_idx: single numerical index for array (up to 1024) Returns: channel row and channel index \"\"\" if ch_idx > 1023 or ch_idx < 0 : print ( 'Chan num out of range' ) return - 1 else : ch_row = int ( ch_idx / 32 ) ch_col = int ( ch_idx - ch_row * 32 ) return ch_row , ch_col init_data_loading ( path ) Parameters: Name Type Description Default path str required Source code in src/model/data_loading.py def init_data_loading ( path : str ): \"\"\" Args: path: Returns: \"\"\" bufDir = os . listdir ( path ) num_of_buf = len ( bufDir ) bramdepth = 65536 datarun = os . path . basename ( path ) datapiece = os . path . basename ( os . path . dirname ( path )) # initialize variables dataAll = np . zeros (( 32 , 32 , int ( bramdepth * num_of_buf / 2 ))) # Largest possible value of dataAll, perfect recording, only double cnt cntAll = np . zeros (( 32 , 32 , int ( bramdepth * num_of_buf / 2 ))) times = np . zeros (( int ( bramdepth * num_of_buf / 2 ))) loadingDict = { \"path\" : path , \"datarun\" : datarun , \"datapiece\" : datapiece , \"bufDir\" : bufDir , \"num_of_buf\" : num_of_buf , \"bramDepth\" : bramdepth , \"dataAll\" : dataAll , \"cntAll\" : cntAll , \"times\" : times } return loadingDict load_first_buffer_info ( app ) Parameters: Name Type Description Default app required Source code in src/model/data_loading.py def load_first_buffer_info ( app ): \"\"\" Args: app: Returns: \"\"\" data_run = os . path . basename ( app . settings [ \"path\" ]) file_dir = app . settings [ \"path\" ] + \"/\" + data_run + \"_\" + str ( 0 ) + \".mat\" first_file_params = { \"file_dir\" : file_dir , \"filter_type\" : app . settings [ \"filter\" ], \"packet_idx\" : 0 , \"SPIKING_THRESHOLD\" : app . settings [ \"spikeThreshold\" ], \"BIN_SIZE\" : app . settings [ \"binSize\" ] } packet = load_one_mat_file ( first_file_params ) app . data . to_serialize . put ( packet ) app . curr_buf_idx = 1 NUM_CHANNELS_PER_BUFFER = len ( packet [ \"packet_data\" ]) return NUM_CHANNELS_PER_BUFFER load_one_mat_file ( params ) Parameters: Name Type Description Default params required Source code in src/model/data_loading.py def load_one_mat_file ( params ): \"\"\" Args: params: Returns: \"\"\" # this is designed to be multi-processed file_dir = params [ \"file_dir\" ] filter_type = params [ \"filter_type\" ] SPIKING_THRESHOLD = params [ \"SPIKING_THRESHOLD\" ] BIN_SIZE = params [ \"BIN_SIZE\" ] mat_contents = sio . loadmat ( file_dir ) dataRaw = mat_contents [ 'gmem1' ][ 0 ][:] from src.model.raw_data_helpers import removeMultipleCounts data_real , cnt_real , N = removeMultipleCounts ( dataRaw ) # Note: this code does not timestamp model bc that cannot be parallelized properly packet_data = preprocess_raw_data ( data_real , cnt_real , N ) packet = { \"packet_data\" : packet_data , \"packet_idx\" : params [ \"packet_idx\" ], \"file_dir\" : params [ \"file_dir\" ], \"filter_type\" : params [ \"filter_type\" ] } from src.model.filters import filter_preprocessed_data packet = filter_preprocessed_data ( packet , filter_type = filter_type ) from src.model.statistics import calculate_channel_stats packet = calculate_channel_stats ( packet , SPIKING_THRESHOLD , BIN_SIZE ) return packet map2idx ( ch_row , ch_col ) Given a channel's row and col, return channel's index Parameters: Name Type Description Default ch_row int row index of channel in array (up to 32) required ch_col int column index of channel in array (up to 32) required Returns: numerical index of array Source code in src/model/data_loading.py def map2idx ( ch_row : int , ch_col : int ): \"\"\" Given a channel's row and col, return channel's index Args: ch_row: row index of channel in array (up to 32) ch_col: column index of channel in array (up to 32) Returns: numerical index of array \"\"\" if ch_row > 31 or ch_row < 0 : print ( 'Row out of range' ) elif ch_col > 31 or ch_col < 0 : print ( 'Col out of range' ) else : ch_idx = int ( ch_row * 32 + ch_col ) return ch_idx preprocess_raw_data ( data_real , cnt_real , N , SAMPLING_PERIOD = 0.05 ) Parameters: Name Type Description Default data_real required cnt_real required N required SAMPLING_PERIOD 0.05 Source code in src/model/data_loading.py def preprocess_raw_data ( data_real , cnt_real , N , SAMPLING_PERIOD = 0.05 ): \"\"\" Args: data_real: cnt_real: N: SAMPLING_PERIOD: Returns: \"\"\" # Determine time estimate and sample counts for the total combined buffers # (note this does not take into account communication delays - hence an estimate) end_time = N * SAMPLING_PERIOD # 20kHz sampling rate, means time_recording (ms) = num_sam * 0.05ms # we are not determining absolute time, because we want to parallelize this, # and time tracking must be done in sequence times = np . linspace ( 0 , end_time , N + 1 ) # identify relevant, nonzero channels, and then append only this model into recorded_data from src.model.raw_data_helpers import identify_relevant_channels num_channels , channel_map , channel_id , start_idx , find_coords , recorded_channels = identify_relevant_channels ( data_real ) packet_data = [] for i in range ( recorded_channels . shape [ 0 ]): # process each channel index channel_idx = int ( recorded_channels [ i ][ 0 ]) x , y = idx2map ( channel_idx ) start_idx = recorded_channels [ i ][ 0 ] channel_data = data_real [ x , y , start_idx : N ] # channel_times = self.times[self.count_track + start_idx: self.count_track+N] can't do # prune the times in the packet where nothing is recorded (aka when model == 0) # TODO turn on # actual_recording_times = (channel_data != 0] # channel_times = channel_times[actual_recording_times] # channel_data = channel_data[actual_recording_times] channel_data = { \"data_real\" : data_real , \"cnt_real\" : cnt_real , \"N\" : N , \"channel_idx\" : channel_idx , \"preprocessed_data\" : channel_data } packet_data . append ( channel_data ) return packet_data","title":"data loading"},{"location":"model/data_loading/#data-loading","text":"Functions for loading model files into form that can be read by GUI","title":"Data Loading"},{"location":"model/data_loading/#src.model.data_loading.idx2map","text":"Given a channel index, return the channel's row and col Parameters: Name Type Description Default ch_idx int single numerical index for array (up to 1024) required Returns: Type Description channel row and channel index Source code in src/model/data_loading.py def idx2map ( ch_idx : int ): \"\"\" Given a channel index, return the channel's row and col Args: ch_idx: single numerical index for array (up to 1024) Returns: channel row and channel index \"\"\" if ch_idx > 1023 or ch_idx < 0 : print ( 'Chan num out of range' ) return - 1 else : ch_row = int ( ch_idx / 32 ) ch_col = int ( ch_idx - ch_row * 32 ) return ch_row , ch_col","title":"idx2map()"},{"location":"model/data_loading/#src.model.data_loading.init_data_loading","text":"Parameters: Name Type Description Default path str required Source code in src/model/data_loading.py def init_data_loading ( path : str ): \"\"\" Args: path: Returns: \"\"\" bufDir = os . listdir ( path ) num_of_buf = len ( bufDir ) bramdepth = 65536 datarun = os . path . basename ( path ) datapiece = os . path . basename ( os . path . dirname ( path )) # initialize variables dataAll = np . zeros (( 32 , 32 , int ( bramdepth * num_of_buf / 2 ))) # Largest possible value of dataAll, perfect recording, only double cnt cntAll = np . zeros (( 32 , 32 , int ( bramdepth * num_of_buf / 2 ))) times = np . zeros (( int ( bramdepth * num_of_buf / 2 ))) loadingDict = { \"path\" : path , \"datarun\" : datarun , \"datapiece\" : datapiece , \"bufDir\" : bufDir , \"num_of_buf\" : num_of_buf , \"bramDepth\" : bramdepth , \"dataAll\" : dataAll , \"cntAll\" : cntAll , \"times\" : times } return loadingDict","title":"init_data_loading()"},{"location":"model/data_loading/#src.model.data_loading.load_first_buffer_info","text":"Parameters: Name Type Description Default app required Source code in src/model/data_loading.py def load_first_buffer_info ( app ): \"\"\" Args: app: Returns: \"\"\" data_run = os . path . basename ( app . settings [ \"path\" ]) file_dir = app . settings [ \"path\" ] + \"/\" + data_run + \"_\" + str ( 0 ) + \".mat\" first_file_params = { \"file_dir\" : file_dir , \"filter_type\" : app . settings [ \"filter\" ], \"packet_idx\" : 0 , \"SPIKING_THRESHOLD\" : app . settings [ \"spikeThreshold\" ], \"BIN_SIZE\" : app . settings [ \"binSize\" ] } packet = load_one_mat_file ( first_file_params ) app . data . to_serialize . put ( packet ) app . curr_buf_idx = 1 NUM_CHANNELS_PER_BUFFER = len ( packet [ \"packet_data\" ]) return NUM_CHANNELS_PER_BUFFER","title":"load_first_buffer_info()"},{"location":"model/data_loading/#src.model.data_loading.load_one_mat_file","text":"Parameters: Name Type Description Default params required Source code in src/model/data_loading.py def load_one_mat_file ( params ): \"\"\" Args: params: Returns: \"\"\" # this is designed to be multi-processed file_dir = params [ \"file_dir\" ] filter_type = params [ \"filter_type\" ] SPIKING_THRESHOLD = params [ \"SPIKING_THRESHOLD\" ] BIN_SIZE = params [ \"BIN_SIZE\" ] mat_contents = sio . loadmat ( file_dir ) dataRaw = mat_contents [ 'gmem1' ][ 0 ][:] from src.model.raw_data_helpers import removeMultipleCounts data_real , cnt_real , N = removeMultipleCounts ( dataRaw ) # Note: this code does not timestamp model bc that cannot be parallelized properly packet_data = preprocess_raw_data ( data_real , cnt_real , N ) packet = { \"packet_data\" : packet_data , \"packet_idx\" : params [ \"packet_idx\" ], \"file_dir\" : params [ \"file_dir\" ], \"filter_type\" : params [ \"filter_type\" ] } from src.model.filters import filter_preprocessed_data packet = filter_preprocessed_data ( packet , filter_type = filter_type ) from src.model.statistics import calculate_channel_stats packet = calculate_channel_stats ( packet , SPIKING_THRESHOLD , BIN_SIZE ) return packet","title":"load_one_mat_file()"},{"location":"model/data_loading/#src.model.data_loading.map2idx","text":"Given a channel's row and col, return channel's index Parameters: Name Type Description Default ch_row int row index of channel in array (up to 32) required ch_col int column index of channel in array (up to 32) required Returns: numerical index of array Source code in src/model/data_loading.py def map2idx ( ch_row : int , ch_col : int ): \"\"\" Given a channel's row and col, return channel's index Args: ch_row: row index of channel in array (up to 32) ch_col: column index of channel in array (up to 32) Returns: numerical index of array \"\"\" if ch_row > 31 or ch_row < 0 : print ( 'Row out of range' ) elif ch_col > 31 or ch_col < 0 : print ( 'Col out of range' ) else : ch_idx = int ( ch_row * 32 + ch_col ) return ch_idx","title":"map2idx()"},{"location":"model/data_loading/#src.model.data_loading.preprocess_raw_data","text":"Parameters: Name Type Description Default data_real required cnt_real required N required SAMPLING_PERIOD 0.05 Source code in src/model/data_loading.py def preprocess_raw_data ( data_real , cnt_real , N , SAMPLING_PERIOD = 0.05 ): \"\"\" Args: data_real: cnt_real: N: SAMPLING_PERIOD: Returns: \"\"\" # Determine time estimate and sample counts for the total combined buffers # (note this does not take into account communication delays - hence an estimate) end_time = N * SAMPLING_PERIOD # 20kHz sampling rate, means time_recording (ms) = num_sam * 0.05ms # we are not determining absolute time, because we want to parallelize this, # and time tracking must be done in sequence times = np . linspace ( 0 , end_time , N + 1 ) # identify relevant, nonzero channels, and then append only this model into recorded_data from src.model.raw_data_helpers import identify_relevant_channels num_channels , channel_map , channel_id , start_idx , find_coords , recorded_channels = identify_relevant_channels ( data_real ) packet_data = [] for i in range ( recorded_channels . shape [ 0 ]): # process each channel index channel_idx = int ( recorded_channels [ i ][ 0 ]) x , y = idx2map ( channel_idx ) start_idx = recorded_channels [ i ][ 0 ] channel_data = data_real [ x , y , start_idx : N ] # channel_times = self.times[self.count_track + start_idx: self.count_track+N] can't do # prune the times in the packet where nothing is recorded (aka when model == 0) # TODO turn on # actual_recording_times = (channel_data != 0] # channel_times = channel_times[actual_recording_times] # channel_data = channel_data[actual_recording_times] channel_data = { \"data_real\" : data_real , \"cnt_real\" : cnt_real , \"N\" : N , \"channel_idx\" : channel_idx , \"preprocessed_data\" : channel_data } packet_data . append ( channel_data ) return packet_data","title":"preprocess_raw_data()"},{"location":"model/filters/","text":"Filters applyFilterAuto ( channel_data , dataFilt ) Source code in src/model/filters.py def applyFilterAuto ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" samFreq = 20e3 * 1.0 passband = [ 250 , 4000 ] stopband = [ 5 , 6000 ] max_loss_passband = 3 min_loss_stopband = 30 order , normal_cutoff = signal . buttord ( passband , stopband , max_loss_passband , min_loss_stopband , fs = samFreq ) b , a = signal . butter ( order , normal_cutoff , btype = 'bandpass' , fs = samFreq ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt applyFilterAutoTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterAutoTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" samFreq = 20e3 * 1.0 passband = [ 250 , 4000 ] stopband = [ 5 , 6000 ] max_loss_passband = 3 min_loss_stopband = 30 order , normal_cutoff = signal . buttord ( passband , stopband , max_loss_passband , min_loss_stopband , fs = samFreq ) b , a = signal . butter ( order , normal_cutoff , btype = 'bandpass' , fs = samFreq ) print ( 'Order = ' + str ( order )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterFastBandpass ( channel_data , dataFilt ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterFastBandpass ( channel_data , dataFilt ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt applyFilterFastBandpassTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterFastBandpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , analog = False ) print ( 'Order = ' + str ( 1 )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterFasterBandpass ( channel_data , dataFilt ) Source code in src/model/filters.py def applyFilterFasterBandpass ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq sos1 = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , output = 'sos' ) dataFilt = signal . sosfiltfilt ( sos1 , channel_data ) return dataFilt applyFilterFasterBandpassTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterFasterBandpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq sos1 = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , output = 'sos' ) print ( 'Order = ' + str ( 1 )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . sosfiltfilt ( sos1 , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterH0bandpass ( channel_data , dataFilt ) Source code in src/model/filters.py def applyFilterH0bandpass ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 5 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt applyFilterH0bandpassTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterH0bandpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 5 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) print ( 'Order = ' + str ( 5 )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterHierlemann ( channel_data , dataFilt ) Source code in src/model/filters.py def applyFilterHierlemann ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" BP_LOW_CUTOFF = 100.0 NUM_TAPS = 75 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 dataFilt = signal . filtfilt ( TAPS , [ a ], channel_data ) return dataFilt applyFilterHierlemannTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterHierlemannTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" BP_LOW_CUTOFF = 100.0 NUM_TAPS = 75 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( TAPS , [ a ], dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterHighpass ( channel_data , dataFilt ) Source code in src/model/filters.py def applyFilterHighpass ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff = 250 / nyq b , a = signal . butter ( 5 , [ cutoff ], btype = \"highpass\" , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt applyFilterHighpassTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterHighpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff = 250 / nyq b , a = signal . butter ( 5 , [ cutoff ], btype = \"highpass\" , analog = False ) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterLitke ( channel_data , dataFilt ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterLitke ( channel_data , dataFilt ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 2000 / nyq b , a = signal . butter ( 2 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt applyFilterLitkeTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterLitkeTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 2000 / nyq b , a = signal . butter ( 2 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterModHierlemann ( channel_data , dataFilt ) Parameters: Name Type Description Default channel_data required dataFilt required Source code in src/model/filters.py def applyFilterModHierlemann ( channel_data , dataFilt ): \"\"\" Args: channel_data: dataFilt: Returns: \"\"\" BP_LOW_CUTOFF = 250.0 BP_HIGH_CUTOFF = 4000.0 NUM_TAPS = 100 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , BP_HIGH_CUTOFF ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 dataFilt = signal . filtfilt ( TAPS , [ a ], channel_data ) return dataFilt applyFilterModHierlemannTimed ( dataAll , dataFilt , numChan , chMap ) Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterModHierlemannTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" BP_LOW_CUTOFF = 250.0 BP_HIGH_CUTOFF = 4000.0 NUM_TAPS = 100 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , BP_HIGH_CUTOFF ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( TAPS , [ a ], dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt applyFilterToAllData ( dataAll , numChan , chMap , filtType = 'Modified Hierlemann' , debug = False ) Parameters: Name Type Description Default dataAll required numChan required chMap required filtType 'Modified Hierlemann' Source code in src/model/filters.py def applyFilterToAllData ( dataAll , numChan , chMap , filtType = 'Modified Hierlemann' , debug = False ): \"\"\" Args: dataAll: numChan: chMap: filtType: Returns: \"\"\" # Future update: only calculate for channels recorded not all dataFilt = np . zeros (( np . shape ( dataAll ))) if filtType == 'Hierlemann' : dataFilt = applyFilterHierlemannTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Modified Hierlemann' : dataFilt = applyFilterModHierlemannTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Highpass' : dataFilt = applyFilterHighpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'H0 Bandpass' : dataFilt = applyFilterH0bandpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Auto' : dataFilt = applyFilterAutoTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Fast Bandpass' : dataFilt = applyFilterFastBandpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Faster Bandpass' : dataFilt = applyFilterFasterBandpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Litke' : dataFilt = applyFilterLitkeTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'None' : dataFilt = np . copy ( dataAll ) else : dataFilt = np . copy ( dataAll ) print ( 'Filter not recognized. Options include Hierlemann, highpass, bandpass, Litke or none' ) if debug : print ( \"Filter type: \" + str ( filtType )) return dataFilt applyFilterToChannelData ( channel_data , filtType = 'Hierlemann' , debug = False ) Parameters: Name Type Description Default channel_data required filtType 'Hierlemann' Source code in src/model/filters.py def applyFilterToChannelData ( channel_data , filtType = 'Hierlemann' , debug = False ): \"\"\" Args: channel_data: filtType: Returns: \"\"\" dataFilt = np . zeros (( np . shape ( channel_data ))) if filtType == 'Hierlemann' : dataFilt = applyFilterHierlemann ( channel_data , dataFilt ) elif filtType == 'Modified Hierlemann' : dataFilt = applyFilterModHierlemann ( channel_data , dataFilt ) elif filtType == 'Highpass' : dataFilt = applyFilterHighpass ( channel_data , dataFilt ) elif filtType == 'H0 Bandpass' : dataFilt = applyFilterH0bandpass ( channel_data , dataFilt ) elif filtType == 'Auto' : dataFilt = applyFilterAuto ( channel_data , dataFilt ) elif filtType == 'Fast Bandpass' : dataFilt = applyFilterFastBandpass ( channel_data , dataFilt ) elif filtType == 'Faster Bandpass' : dataFilt = applyFilterFasterBandpass ( channel_data , dataFilt ) elif filtType == 'Litke' : dataFilt = applyFilterLitke ( channel_data , dataFilt ) elif filtType == 'None' : dataFilt = np . copy ( channel_data ) else : dataFilt = np . copy ( channel_data ) print ( 'Filter type \"' + str ( filtType ) + '\" not recognized. Options include Hierlemann, highpass, bandpass, Litke or none' ) if debug : print ( \"Filter type: \" + str ( filtType )) return dataFilt","title":"filters"},{"location":"model/filters/#filters","text":"","title":"Filters"},{"location":"model/filters/#src.model.filters.applyFilterAuto","text":"Source code in src/model/filters.py def applyFilterAuto ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" samFreq = 20e3 * 1.0 passband = [ 250 , 4000 ] stopband = [ 5 , 6000 ] max_loss_passband = 3 min_loss_stopband = 30 order , normal_cutoff = signal . buttord ( passband , stopband , max_loss_passband , min_loss_stopband , fs = samFreq ) b , a = signal . butter ( order , normal_cutoff , btype = 'bandpass' , fs = samFreq ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt","title":"applyFilterAuto()"},{"location":"model/filters/#src.model.filters.applyFilterAutoTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterAutoTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" samFreq = 20e3 * 1.0 passband = [ 250 , 4000 ] stopband = [ 5 , 6000 ] max_loss_passband = 3 min_loss_stopband = 30 order , normal_cutoff = signal . buttord ( passband , stopband , max_loss_passband , min_loss_stopband , fs = samFreq ) b , a = signal . butter ( order , normal_cutoff , btype = 'bandpass' , fs = samFreq ) print ( 'Order = ' + str ( order )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterAutoTimed()"},{"location":"model/filters/#src.model.filters.applyFilterFastBandpass","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterFastBandpass ( channel_data , dataFilt ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt","title":"applyFilterFastBandpass()"},{"location":"model/filters/#src.model.filters.applyFilterFastBandpassTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterFastBandpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , analog = False ) print ( 'Order = ' + str ( 1 )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterFastBandpassTimed()"},{"location":"model/filters/#src.model.filters.applyFilterFasterBandpass","text":"Source code in src/model/filters.py def applyFilterFasterBandpass ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq sos1 = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , output = 'sos' ) dataFilt = signal . sosfiltfilt ( sos1 , channel_data ) return dataFilt","title":"applyFilterFasterBandpass()"},{"location":"model/filters/#src.model.filters.applyFilterFasterBandpassTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterFasterBandpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq sos1 = signal . butter ( 1 , [ cutoff1 , cutoff2 ], btype = 'bandpass' , output = 'sos' ) print ( 'Order = ' + str ( 1 )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . sosfiltfilt ( sos1 , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterFasterBandpassTimed()"},{"location":"model/filters/#src.model.filters.applyFilterH0bandpass","text":"Source code in src/model/filters.py def applyFilterH0bandpass ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 5 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt","title":"applyFilterH0bandpass()"},{"location":"model/filters/#src.model.filters.applyFilterH0bandpassTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterH0bandpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 4000 / nyq b , a = signal . butter ( 5 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) print ( 'Order = ' + str ( 5 )) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterH0bandpassTimed()"},{"location":"model/filters/#src.model.filters.applyFilterHierlemann","text":"Source code in src/model/filters.py def applyFilterHierlemann ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" BP_LOW_CUTOFF = 100.0 NUM_TAPS = 75 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 dataFilt = signal . filtfilt ( TAPS , [ a ], channel_data ) return dataFilt","title":"applyFilterHierlemann()"},{"location":"model/filters/#src.model.filters.applyFilterHierlemannTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterHierlemannTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" BP_LOW_CUTOFF = 100.0 NUM_TAPS = 75 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( TAPS , [ a ], dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterHierlemannTimed()"},{"location":"model/filters/#src.model.filters.applyFilterHighpass","text":"Source code in src/model/filters.py def applyFilterHighpass ( channel_data , dataFilt ): \"\"\" Args: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff = 250 / nyq b , a = signal . butter ( 5 , [ cutoff ], btype = \"highpass\" , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt","title":"applyFilterHighpass()"},{"location":"model/filters/#src.model.filters.applyFilterHighpassTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterHighpassTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff = 250 / nyq b , a = signal . butter ( 5 , [ cutoff ], btype = \"highpass\" , analog = False ) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterHighpassTimed()"},{"location":"model/filters/#src.model.filters.applyFilterLitke","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterLitke ( channel_data , dataFilt ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 2000 / nyq b , a = signal . butter ( 2 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) dataFilt = signal . filtfilt ( b , a , channel_data ) return dataFilt","title":"applyFilterLitke()"},{"location":"model/filters/#src.model.filters.applyFilterLitkeTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterLitkeTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" nyq = 0.5 * ( 20e3 * 1.0 ) cutoff1 = 250 / nyq cutoff2 = 2000 / nyq b , a = signal . butter ( 2 , [ cutoff1 , cutoff2 ], btype = \"bandpass\" , analog = False ) for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( b , a , dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterLitkeTimed()"},{"location":"model/filters/#src.model.filters.applyFilterModHierlemann","text":"Parameters: Name Type Description Default channel_data required dataFilt required Source code in src/model/filters.py def applyFilterModHierlemann ( channel_data , dataFilt ): \"\"\" Args: channel_data: dataFilt: Returns: \"\"\" BP_LOW_CUTOFF = 250.0 BP_HIGH_CUTOFF = 4000.0 NUM_TAPS = 100 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , BP_HIGH_CUTOFF ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 dataFilt = signal . filtfilt ( TAPS , [ a ], channel_data ) return dataFilt","title":"applyFilterModHierlemann()"},{"location":"model/filters/#src.model.filters.applyFilterModHierlemannTimed","text":"Parameters: Name Type Description Default dataAll required dataFilt required numChan required chMap required Source code in src/model/filters.py def applyFilterModHierlemannTimed ( dataAll , dataFilt , numChan , chMap ): \"\"\" Args: dataAll: dataFilt: numChan: chMap: Returns: \"\"\" BP_LOW_CUTOFF = 250.0 BP_HIGH_CUTOFF = 4000.0 NUM_TAPS = 100 TAPS = signal . firwin ( NUM_TAPS , [ BP_LOW_CUTOFF , BP_HIGH_CUTOFF ], pass_zero = False , fs = 20e3 * 1.0 ) a = 1 for k in range ( 0 , numChan ): start = time . time () dataFilt [ chMap [ 0 , k ], chMap [ 1 , k ], :] = signal . filtfilt ( TAPS , [ a ], dataAll [ chMap [ 0 , k ], chMap [ 1 , k ], :]) end = time . time () text1 = 'Estimated Time Remaining: ' + str . format ( ' {0:.2f} ' , ( end - start ) * ( numChan - k ) / 60 ) + ' min' text2 = str ( k + 1 ) + '/' + str ( numChan ) + ' Channels Filtered' print ( text1 + ' ' + text2 , end = \" \\r \" ) return dataFilt","title":"applyFilterModHierlemannTimed()"},{"location":"model/filters/#src.model.filters.applyFilterToAllData","text":"Parameters: Name Type Description Default dataAll required numChan required chMap required filtType 'Modified Hierlemann' Source code in src/model/filters.py def applyFilterToAllData ( dataAll , numChan , chMap , filtType = 'Modified Hierlemann' , debug = False ): \"\"\" Args: dataAll: numChan: chMap: filtType: Returns: \"\"\" # Future update: only calculate for channels recorded not all dataFilt = np . zeros (( np . shape ( dataAll ))) if filtType == 'Hierlemann' : dataFilt = applyFilterHierlemannTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Modified Hierlemann' : dataFilt = applyFilterModHierlemannTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Highpass' : dataFilt = applyFilterHighpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'H0 Bandpass' : dataFilt = applyFilterH0bandpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Auto' : dataFilt = applyFilterAutoTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Fast Bandpass' : dataFilt = applyFilterFastBandpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Faster Bandpass' : dataFilt = applyFilterFasterBandpassTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'Litke' : dataFilt = applyFilterLitkeTimed ( dataAll , dataFilt , numChan , chMap ) elif filtType == 'None' : dataFilt = np . copy ( dataAll ) else : dataFilt = np . copy ( dataAll ) print ( 'Filter not recognized. Options include Hierlemann, highpass, bandpass, Litke or none' ) if debug : print ( \"Filter type: \" + str ( filtType )) return dataFilt","title":"applyFilterToAllData()"},{"location":"model/filters/#src.model.filters.applyFilterToChannelData","text":"Parameters: Name Type Description Default channel_data required filtType 'Hierlemann' Source code in src/model/filters.py def applyFilterToChannelData ( channel_data , filtType = 'Hierlemann' , debug = False ): \"\"\" Args: channel_data: filtType: Returns: \"\"\" dataFilt = np . zeros (( np . shape ( channel_data ))) if filtType == 'Hierlemann' : dataFilt = applyFilterHierlemann ( channel_data , dataFilt ) elif filtType == 'Modified Hierlemann' : dataFilt = applyFilterModHierlemann ( channel_data , dataFilt ) elif filtType == 'Highpass' : dataFilt = applyFilterHighpass ( channel_data , dataFilt ) elif filtType == 'H0 Bandpass' : dataFilt = applyFilterH0bandpass ( channel_data , dataFilt ) elif filtType == 'Auto' : dataFilt = applyFilterAuto ( channel_data , dataFilt ) elif filtType == 'Fast Bandpass' : dataFilt = applyFilterFastBandpass ( channel_data , dataFilt ) elif filtType == 'Faster Bandpass' : dataFilt = applyFilterFasterBandpass ( channel_data , dataFilt ) elif filtType == 'Litke' : dataFilt = applyFilterLitke ( channel_data , dataFilt ) elif filtType == 'None' : dataFilt = np . copy ( channel_data ) else : dataFilt = np . copy ( channel_data ) print ( 'Filter type \"' + str ( filtType ) + '\" not recognized. Options include Hierlemann, highpass, bandpass, Litke or none' ) if debug : print ( \"Filter type: \" + str ( filtType )) return dataFilt","title":"applyFilterToChannelData()"},{"location":"model/raw_data_helpers/","text":"Raw Data Helpers identify_relevant_channels ( raw_data ) Only n number of channels can be recorded at a given time due to bandwidth concerns--the rest are shut off. For given model recorded at during a certain window, find which channels were recorded. Parameters: Name Type Description Default raw_data_all unprocessed model in a given time window (num_channels_X x num_channels_Y x time_len) required Returns: Type Description num_channels number of channels that were found to have been recorded for given model channel_map: list of channels that were recorded, i.e. nonzero (x/y_coords, num_channels) channel_id: list of channels that were recorded, but identified by a single numerical ID start_idx: @param raw_data: all the raw model loaded thus far from files Source code in src/model/raw_data_helpers.py def identify_relevant_channels ( raw_data : np . array ): \"\"\" Only n number of channels can be recorded at a given time due to bandwidth concerns--the rest are shut off. For given model recorded at during a certain window, find which channels were recorded. Args: raw_data_all: unprocessed model in a given time window (num_channels_X x num_channels_Y x time_len) Returns: num_channels: number of channels that were found to have been recorded for given model channel_map: list of channels that were recorded, i.e. nonzero (x/y_coords, num_channels) channel_id: list of channels that were recorded, but identified by a single numerical ID start_idx: @param raw_data: all the raw model loaded thus far from files \"\"\" # This bit takes the longest. ~ 30 sec for whole array 4 channel recording num_samples = np . count_nonzero ( raw_data , axis = 2 ) num_channels = np . count_nonzero ( num_samples ) # Map and Identify recorded channels find_coords = np . nonzero ( num_samples ) channel_map = np . array ( find_coords ) channel_id = np . zeros ( num_channels ) start_idx = np . zeros ( num_channels ) from src.model.data_loading import map2idx for k in range ( 0 , num_channels ): channel_id [ k ] = map2idx ( channel_map [ 0 , k ], channel_map [ 1 , k ]) start_idx [ k ] = ( raw_data [ channel_map [ 0 , k ], channel_map [ 1 , k ], :] != 0 ) . argmax ( axis = 0 ) recorded_channels = np . stack (( channel_id , start_idx ), axis = 1 ) . astype ( int ) #return recorded_channels return num_channels , channel_map , channel_id , start_idx , find_coords , recorded_channels removeMultipleCounts ( dataRaw ) Parameters: Name Type Description Default dataRaw required Source code in src/model/raw_data_helpers.py def removeMultipleCounts ( dataRaw ): \"\"\" Args: dataRaw: Returns: \"\"\" # Initialize Variables Needed for Each Buffer chan_index_pre = 1025 # Check for chan changes, double cnt cnt_pre = 0 # Check for cnt changes, double cnt N = 0 # Sample times (DOES NOT ALLOW NON-COLLISION FREE SAMPLES) data_real = np . zeros ( ( 32 , 32 , len ( dataRaw ) - 2 )) # Initialize to max possible length. Note: Throw out first two values b/c garbo cnt_real = np . zeros (( 32 , 32 , len ( dataRaw ) - 2 )) # Convert model and remove double/triple counts for i in range ( 2 , len ( dataRaw ) - 1 ): # Convert bit number into binary word = ( np . binary_repr ( dataRaw [ i ], 32 )) # Break that binary into it's respective pieces and convert to bit number cnt = int ( word [ 12 : 14 ], 2 ) col = int ( word [ 27 : 32 ], 2 ) row = int ( word [ 22 : 27 ], 2 ) chan_index = row * 32 + col # Only record the unique non-double count sample if ( i == 2 or ( cnt_pre != cnt or chan_index != chan_index_pre )): # Sample time only changes when cnt changes if cnt != cnt_pre : N += 1 # On the occurance the first cnt is not 0, make sure sample time is 0 if i == 2 : N = 0 # Update variables cnt_pre = cnt chan_index_pre = chan_index # Record pertinent model data_real [ row ][ col ][ N ] = int ( word [ 14 : 22 ], 2 ) cnt_real [ row ][ col ][ N ] = cnt return data_real , cnt_real , N","title":"raw data helpers"},{"location":"model/raw_data_helpers/#raw-data-helpers","text":"","title":"Raw Data Helpers"},{"location":"model/raw_data_helpers/#src.model.raw_data_helpers.identify_relevant_channels","text":"Only n number of channels can be recorded at a given time due to bandwidth concerns--the rest are shut off. For given model recorded at during a certain window, find which channels were recorded. Parameters: Name Type Description Default raw_data_all unprocessed model in a given time window (num_channels_X x num_channels_Y x time_len) required Returns: Type Description num_channels number of channels that were found to have been recorded for given model channel_map: list of channels that were recorded, i.e. nonzero (x/y_coords, num_channels) channel_id: list of channels that were recorded, but identified by a single numerical ID start_idx: @param raw_data: all the raw model loaded thus far from files Source code in src/model/raw_data_helpers.py def identify_relevant_channels ( raw_data : np . array ): \"\"\" Only n number of channels can be recorded at a given time due to bandwidth concerns--the rest are shut off. For given model recorded at during a certain window, find which channels were recorded. Args: raw_data_all: unprocessed model in a given time window (num_channels_X x num_channels_Y x time_len) Returns: num_channels: number of channels that were found to have been recorded for given model channel_map: list of channels that were recorded, i.e. nonzero (x/y_coords, num_channels) channel_id: list of channels that were recorded, but identified by a single numerical ID start_idx: @param raw_data: all the raw model loaded thus far from files \"\"\" # This bit takes the longest. ~ 30 sec for whole array 4 channel recording num_samples = np . count_nonzero ( raw_data , axis = 2 ) num_channels = np . count_nonzero ( num_samples ) # Map and Identify recorded channels find_coords = np . nonzero ( num_samples ) channel_map = np . array ( find_coords ) channel_id = np . zeros ( num_channels ) start_idx = np . zeros ( num_channels ) from src.model.data_loading import map2idx for k in range ( 0 , num_channels ): channel_id [ k ] = map2idx ( channel_map [ 0 , k ], channel_map [ 1 , k ]) start_idx [ k ] = ( raw_data [ channel_map [ 0 , k ], channel_map [ 1 , k ], :] != 0 ) . argmax ( axis = 0 ) recorded_channels = np . stack (( channel_id , start_idx ), axis = 1 ) . astype ( int ) #return recorded_channels return num_channels , channel_map , channel_id , start_idx , find_coords , recorded_channels","title":"identify_relevant_channels()"},{"location":"model/raw_data_helpers/#src.model.raw_data_helpers.removeMultipleCounts","text":"Parameters: Name Type Description Default dataRaw required Source code in src/model/raw_data_helpers.py def removeMultipleCounts ( dataRaw ): \"\"\" Args: dataRaw: Returns: \"\"\" # Initialize Variables Needed for Each Buffer chan_index_pre = 1025 # Check for chan changes, double cnt cnt_pre = 0 # Check for cnt changes, double cnt N = 0 # Sample times (DOES NOT ALLOW NON-COLLISION FREE SAMPLES) data_real = np . zeros ( ( 32 , 32 , len ( dataRaw ) - 2 )) # Initialize to max possible length. Note: Throw out first two values b/c garbo cnt_real = np . zeros (( 32 , 32 , len ( dataRaw ) - 2 )) # Convert model and remove double/triple counts for i in range ( 2 , len ( dataRaw ) - 1 ): # Convert bit number into binary word = ( np . binary_repr ( dataRaw [ i ], 32 )) # Break that binary into it's respective pieces and convert to bit number cnt = int ( word [ 12 : 14 ], 2 ) col = int ( word [ 27 : 32 ], 2 ) row = int ( word [ 22 : 27 ], 2 ) chan_index = row * 32 + col # Only record the unique non-double count sample if ( i == 2 or ( cnt_pre != cnt or chan_index != chan_index_pre )): # Sample time only changes when cnt changes if cnt != cnt_pre : N += 1 # On the occurance the first cnt is not 0, make sure sample time is 0 if i == 2 : N = 0 # Update variables cnt_pre = cnt chan_index_pre = chan_index # Record pertinent model data_real [ row ][ col ][ N ] = int ( word [ 14 : 22 ], 2 ) cnt_real [ row ][ col ][ N ] = cnt return data_real , cnt_real , N","title":"removeMultipleCounts()"},{"location":"model/spike_detection/","text":"Spike Detection Contains functions for detecting spikes Includes Gaussian Mixture Model (GMM) and Thresholding Huy Nguyen, John Bailey, Maddy Hays (2022) binSpikeTimes ( buf_recording_len , incom_spike_idx , incom_spike_amps , BIN_SIZE ) Parameters: Name Type Description Default buf_recording_len required incom_spike_idx required incom_spike_amps required BIN_SIZE required Source code in src/model/spike_detection.py def binSpikeTimes ( buf_recording_len , incom_spike_idx , incom_spike_amps , BIN_SIZE ): \"\"\" Args: buf_recording_len: incom_spike_idx: incom_spike_amps: BIN_SIZE: Returns: \"\"\" NUM_BINS_IN_BUFFER = math . floor ( buf_recording_len / BIN_SIZE ) # initialize an array of spike bins, with no spikes detected spikeBins = np . zeros ( NUM_BINS_IN_BUFFER , dtype = bool ) spikeBinsMaxAmp = np . zeros ( NUM_BINS_IN_BUFFER , dtype = float ) for bin_idx in range ( int ( buf_recording_len / BIN_SIZE )): bin_start = bin_idx * BIN_SIZE bin_end = ( bin_idx + 1 ) * BIN_SIZE spikes_within_bin = ( bin_start <= incom_spike_idx ) & ( incom_spike_idx < bin_end ) if np . count_nonzero ( spikes_within_bin ) != 0 : spikeBins [ bin_idx ] = True spiking_amps = incom_spike_amps [ spikes_within_bin ] spikeBinsMaxAmp [ bin_idx ] = np . max ( spiking_amps ) return spikeBins , spikeBinsMaxAmp , NUM_BINS_IN_BUFFER findSpikesGMM ( electrode_data , channel_idx , debug = False ) @param electrode_data: Data to apply GMM to (i.e. self.electrode_data in individual channels file) @param chan_idx: Index of electrode channel @param debug: Prints model if True @return: spikeMeanGMM, spikeStdGMM, noiseMeanGMM, noiseStdGMM Source code in src/model/spike_detection.py def findSpikesGMM ( electrode_data , channel_idx , debug = False ): \"\"\" @param electrode_data: Data to apply GMM to (i.e. self.electrode_data in individual channels file) @param chan_idx: Index of electrode channel @param debug: Prints model if True @return: spikeMeanGMM, spikeStdGMM, noiseMeanGMM, noiseStdGMM \"\"\" spikeMeanGMM = 0 noiseMeanGMM = 0 spikeStdGMM = 0 noiseStdGMM = 0 # Get model and perform GM y = electrode_data gmSam = np . reshape ( y ,( len ( y ), 1 )) gm = GaussianMixture ( n_components = 2 ) . fit ( gmSam ) # Get means, weights, st devs means = gm . means_ . flatten () weights = gm . weights_ . flatten () stanDevs = np . sqrt ( gm . covariances_ ) . flatten () # Assign means and standard deviations by finding which index # represents spikes and which noise (noise should have 0 mean) noiseIdx = 0 spikesIdx = 0 for i in range ( len ( means )): if means [ i ] == np . min ( np . abs ( means )): noiseMeanGMM = means [ i ] noiseIdx = i else : spikeMeanGMM = means [ i ] spikesIdx = i spikeStdGMM = stanDevs [ spikesIdx ] noiseStdGMM = stanDevs [ noiseIdx ] if debug : print ( \"Channel for GMM: \" + str ( channel_idx )) print ( \"spike mean: \" + str ( spikeMeanGMM ) + \"| spike std: \" + str ( spikeStdGMM ) + \"| noise mean: \" + str ( noiseMeanGMM ) + \"| noise std: \" + str ( noiseStdGMM )) return spikeMeanGMM , spikeStdGMM , noiseMeanGMM , noiseStdGMM getAboveThresholdActivity ( data , channel_noise_mean , channel_noise_std , spiking_threshold ) Parameters: Name Type Description Default data required channel_noise_mean required channel_noise_std required spiking_threshold required Source code in src/model/spike_detection.py def getAboveThresholdActivity ( data , channel_noise_mean , channel_noise_std , spiking_threshold ): \"\"\" Args: data: channel_noise_mean: channel_noise_std: spiking_threshold: Returns: \"\"\" below_threshold = channel_noise_mean - ( spiking_threshold * channel_noise_std ) above_threshold_activity = ( data <= below_threshold ) incom_spike_idx = np . argwhere ( above_threshold_activity ) . flatten () incom_spike_amplitude = data [ incom_spike_idx ] return incom_spike_idx , incom_spike_amplitude","title":"spike detection"},{"location":"model/spike_detection/#spike-detection","text":"Contains functions for detecting spikes Includes Gaussian Mixture Model (GMM) and Thresholding Huy Nguyen, John Bailey, Maddy Hays (2022)","title":"Spike Detection"},{"location":"model/spike_detection/#src.model.spike_detection.binSpikeTimes","text":"Parameters: Name Type Description Default buf_recording_len required incom_spike_idx required incom_spike_amps required BIN_SIZE required Source code in src/model/spike_detection.py def binSpikeTimes ( buf_recording_len , incom_spike_idx , incom_spike_amps , BIN_SIZE ): \"\"\" Args: buf_recording_len: incom_spike_idx: incom_spike_amps: BIN_SIZE: Returns: \"\"\" NUM_BINS_IN_BUFFER = math . floor ( buf_recording_len / BIN_SIZE ) # initialize an array of spike bins, with no spikes detected spikeBins = np . zeros ( NUM_BINS_IN_BUFFER , dtype = bool ) spikeBinsMaxAmp = np . zeros ( NUM_BINS_IN_BUFFER , dtype = float ) for bin_idx in range ( int ( buf_recording_len / BIN_SIZE )): bin_start = bin_idx * BIN_SIZE bin_end = ( bin_idx + 1 ) * BIN_SIZE spikes_within_bin = ( bin_start <= incom_spike_idx ) & ( incom_spike_idx < bin_end ) if np . count_nonzero ( spikes_within_bin ) != 0 : spikeBins [ bin_idx ] = True spiking_amps = incom_spike_amps [ spikes_within_bin ] spikeBinsMaxAmp [ bin_idx ] = np . max ( spiking_amps ) return spikeBins , spikeBinsMaxAmp , NUM_BINS_IN_BUFFER","title":"binSpikeTimes()"},{"location":"model/spike_detection/#src.model.spike_detection.findSpikesGMM","text":"@param electrode_data: Data to apply GMM to (i.e. self.electrode_data in individual channels file) @param chan_idx: Index of electrode channel @param debug: Prints model if True @return: spikeMeanGMM, spikeStdGMM, noiseMeanGMM, noiseStdGMM Source code in src/model/spike_detection.py def findSpikesGMM ( electrode_data , channel_idx , debug = False ): \"\"\" @param electrode_data: Data to apply GMM to (i.e. self.electrode_data in individual channels file) @param chan_idx: Index of electrode channel @param debug: Prints model if True @return: spikeMeanGMM, spikeStdGMM, noiseMeanGMM, noiseStdGMM \"\"\" spikeMeanGMM = 0 noiseMeanGMM = 0 spikeStdGMM = 0 noiseStdGMM = 0 # Get model and perform GM y = electrode_data gmSam = np . reshape ( y ,( len ( y ), 1 )) gm = GaussianMixture ( n_components = 2 ) . fit ( gmSam ) # Get means, weights, st devs means = gm . means_ . flatten () weights = gm . weights_ . flatten () stanDevs = np . sqrt ( gm . covariances_ ) . flatten () # Assign means and standard deviations by finding which index # represents spikes and which noise (noise should have 0 mean) noiseIdx = 0 spikesIdx = 0 for i in range ( len ( means )): if means [ i ] == np . min ( np . abs ( means )): noiseMeanGMM = means [ i ] noiseIdx = i else : spikeMeanGMM = means [ i ] spikesIdx = i spikeStdGMM = stanDevs [ spikesIdx ] noiseStdGMM = stanDevs [ noiseIdx ] if debug : print ( \"Channel for GMM: \" + str ( channel_idx )) print ( \"spike mean: \" + str ( spikeMeanGMM ) + \"| spike std: \" + str ( spikeStdGMM ) + \"| noise mean: \" + str ( noiseMeanGMM ) + \"| noise std: \" + str ( noiseStdGMM )) return spikeMeanGMM , spikeStdGMM , noiseMeanGMM , noiseStdGMM","title":"findSpikesGMM()"},{"location":"model/spike_detection/#src.model.spike_detection.getAboveThresholdActivity","text":"Parameters: Name Type Description Default data required channel_noise_mean required channel_noise_std required spiking_threshold required Source code in src/model/spike_detection.py def getAboveThresholdActivity ( data , channel_noise_mean , channel_noise_std , spiking_threshold ): \"\"\" Args: data: channel_noise_mean: channel_noise_std: spiking_threshold: Returns: \"\"\" below_threshold = channel_noise_mean - ( spiking_threshold * channel_noise_std ) above_threshold_activity = ( data <= below_threshold ) incom_spike_idx = np . argwhere ( above_threshold_activity ) . flatten () incom_spike_amplitude = data [ incom_spike_idx ] return incom_spike_idx , incom_spike_amplitude","title":"getAboveThresholdActivity()"},{"location":"model/statistics/","text":"Statistics calculate_channel_stats ( packet , SPIKING_THRESHOLD , BIN_SIZE ) Parameters: Name Type Description Default packet required SPIKING_THRESHOLD required BIN_SIZE required Source code in src/model/statistics.py def calculate_channel_stats ( packet , SPIKING_THRESHOLD , BIN_SIZE ): \"\"\" Args: packet: SPIKING_THRESHOLD: BIN_SIZE: Returns: \"\"\" for idx , channel_data in enumerate ( packet [ \"packet_data\" ]): channel_data [ \"stats_avg+unfiltered+amp\" ] = np . mean ( channel_data [ \"preprocessed_data\" ]) channel_data [ \"stats_cnt\" ] = len ( channel_data [ \"filtered_data\" ]) channel_data [ \"stats_noise+mean\" ] = np . mean ( channel_data [ \"filtered_data\" ]) channel_data [ \"stats_noise+std\" ] = np . std ( channel_data [ \"filtered_data\" ]) channel_data [ \"stats_buf+recording+len\" ] = channel_data [ \"stats_cnt\" ] * 0.05 # assuming 20 khZ sampling rate # TODO [later] add GMM spikes SPIKE_DETECTION_METHOD = \"threshold\" if SPIKE_DETECTION_METHOD == \"threshold\" : from src.model.spike_detection import getAboveThresholdActivity , binSpikeTimes incom_spike_idx , incom_spike_amplitude = getAboveThresholdActivity ( channel_data [ \"filtered_data\" ], channel_data [ \"stats_noise+mean\" ], channel_data [ \"stats_noise+std\" ], SPIKING_THRESHOLD ) spikeBins , spikeBinsMaxAmp , NUM_BINS_IN_BUFFER = binSpikeTimes ( channel_data [ \"stats_buf+recording+len\" ], incom_spike_idx , incom_spike_amplitude , BIN_SIZE ) channel_data [ \"stats_spikes+cnt\" ] = sum ( spikeBins ) channel_data [ \"stats_spikes+avg+amp\" ] = np . mean ( spikeBinsMaxAmp ) channel_data [ \"stats_spikes+std\" ] = np . std ( spikeBinsMaxAmp ) channel_data [ \"spike_bins\" ] = spikeBins channel_data [ \"spike_bins_max_amps\" ] = spikeBinsMaxAmp channel_data [ \"stats_num+spike+bins+in+buffer\" ] = NUM_BINS_IN_BUFFER return packet","title":"statistics"},{"location":"model/statistics/#statistics","text":"","title":"Statistics"},{"location":"model/statistics/#src.model.statistics.calculate_channel_stats","text":"Parameters: Name Type Description Default packet required SPIKING_THRESHOLD required BIN_SIZE required Source code in src/model/statistics.py def calculate_channel_stats ( packet , SPIKING_THRESHOLD , BIN_SIZE ): \"\"\" Args: packet: SPIKING_THRESHOLD: BIN_SIZE: Returns: \"\"\" for idx , channel_data in enumerate ( packet [ \"packet_data\" ]): channel_data [ \"stats_avg+unfiltered+amp\" ] = np . mean ( channel_data [ \"preprocessed_data\" ]) channel_data [ \"stats_cnt\" ] = len ( channel_data [ \"filtered_data\" ]) channel_data [ \"stats_noise+mean\" ] = np . mean ( channel_data [ \"filtered_data\" ]) channel_data [ \"stats_noise+std\" ] = np . std ( channel_data [ \"filtered_data\" ]) channel_data [ \"stats_buf+recording+len\" ] = channel_data [ \"stats_cnt\" ] * 0.05 # assuming 20 khZ sampling rate # TODO [later] add GMM spikes SPIKE_DETECTION_METHOD = \"threshold\" if SPIKE_DETECTION_METHOD == \"threshold\" : from src.model.spike_detection import getAboveThresholdActivity , binSpikeTimes incom_spike_idx , incom_spike_amplitude = getAboveThresholdActivity ( channel_data [ \"filtered_data\" ], channel_data [ \"stats_noise+mean\" ], channel_data [ \"stats_noise+std\" ], SPIKING_THRESHOLD ) spikeBins , spikeBinsMaxAmp , NUM_BINS_IN_BUFFER = binSpikeTimes ( channel_data [ \"stats_buf+recording+len\" ], incom_spike_idx , incom_spike_amplitude , BIN_SIZE ) channel_data [ \"stats_spikes+cnt\" ] = sum ( spikeBins ) channel_data [ \"stats_spikes+avg+amp\" ] = np . mean ( spikeBinsMaxAmp ) channel_data [ \"stats_spikes+std\" ] = np . std ( spikeBinsMaxAmp ) channel_data [ \"spike_bins\" ] = spikeBins channel_data [ \"spike_bins_max_amps\" ] = spikeBinsMaxAmp channel_data [ \"stats_num+spike+bins+in+buffer\" ] = NUM_BINS_IN_BUFFER return packet","title":"calculate_channel_stats()"}]}